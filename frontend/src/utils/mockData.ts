import type { RawPaper, XHSContent, PaperCard } from '../types';

// æ¨¡æ‹Ÿçš„è®ºæ–‡æ•°æ®
export const mockPapers: PaperCard[] = [
  {
    raw: {
      id: 'paper_1',
      tweet_id: '1234567890',
      tweet_text: 'ğŸš€ New paper: "Attention Is All You Need" - Introducing Transformer, a new architecture based solely on attention mechanisms. Achieves SOTA on WMT translation! https://arxiv.org/abs/1706.03762',
      tweet_url: 'https://twitter.com/_akhaliq/status/1234567890',
      author_name: 'AK',
      author_username: '_akhaliq',
      like_count: 2850,
      retweet_count: 892,
      reply_count: 156,
      paper_title: 'Attention Is All You Need',
      paper_url: 'https://arxiv.org/abs/1706.03762',
      paper_abstract: 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.',
      created_at: '2024-01-15T10:30:00Z',
      fetched_at: '2024-01-15T12:00:00Z',
    },
  },
  {
    raw: {
      id: 'paper_2',
      tweet_id: '1234567891',
      tweet_text: 'ğŸ”¥ GPT-4 Technical Report is out! Multi-modal model that accepts image and text inputs, produces text outputs. Exhibits human-level performance on many professional benchmarks. https://arxiv.org/abs/2303.08774',
      tweet_url: 'https://twitter.com/_akhaliq/status/1234567891',
      author_name: 'AK',
      author_username: '_akhaliq',
      like_count: 5420,
      retweet_count: 2100,
      reply_count: 380,
      paper_title: 'GPT-4 Technical Report',
      paper_url: 'https://arxiv.org/abs/2303.08774',
      paper_abstract: 'We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks.',
      created_at: '2024-01-14T08:00:00Z',
      fetched_at: '2024-01-14T10:00:00Z',
    },
  },
  {
    raw: {
      id: 'paper_3',
      tweet_id: '1234567892',
      tweet_text: 'âœ¨ Introducing Llama 2: Open Foundation and Fine-Tuned Chat Models. Free for research and commercial use. 7B to 70B parameters! https://arxiv.org/abs/2307.09288',
      tweet_url: 'https://twitter.com/MetaAI/status/1234567892',
      author_name: 'Meta AI',
      author_username: 'MetaAI',
      like_count: 3200,
      retweet_count: 1500,
      reply_count: 220,
      paper_title: 'Llama 2: Open Foundation and Fine-Tuned Chat Models',
      paper_url: 'https://arxiv.org/abs/2307.09288',
      paper_abstract: 'In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.',
      created_at: '2024-01-13T14:00:00Z',
      fetched_at: '2024-01-13T16:00:00Z',
    },
  },
  {
    raw: {
      id: 'paper_4',
      tweet_id: '1234567893',
      tweet_text: 'ğŸ“š "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" - Google AI Language. Sets new SOTA on 11 NLP tasks! https://arxiv.org/abs/1810.04805',
      tweet_url: 'https://twitter.com/GoogleAI/status/1234567893',
      author_name: 'Google AI',
      author_username: 'GoogleAI',
      like_count: 1850,
      retweet_count: 720,
      reply_count: 95,
      paper_title: 'BERT: Pre-training of Deep Bidirectional Transformers',
      paper_url: 'https://arxiv.org/abs/1810.04805',
      paper_abstract: 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text.',
      created_at: '2024-01-12T09:00:00Z',
      fetched_at: '2024-01-12T11:00:00Z',
    },
  },
  {
    raw: {
      id: 'paper_5',
      tweet_id: '1234567894',
      tweet_text: 'ğŸ¨ Stable Diffusion 3 paper dropped! Multi-modal diffusion transformer (MMDiT) architecture achieves incredible image quality. https://arxiv.org/abs/2403.03206',
      tweet_url: 'https://twitter.com/StabilityAI/status/1234567894',
      author_name: 'Stability AI',
      author_username: 'StabilityAI',
      like_count: 4100,
      retweet_count: 1800,
      reply_count: 310,
      paper_title: 'Scaling Rectified Flow Transformers for High-Resolution Image Synthesis',
      paper_url: 'https://arxiv.org/abs/2403.03206',
      paper_abstract: 'We present Stable Diffusion 3, a new family of models using a novel Multimodal Diffusion Transformer (MMDiT) architecture with improved rectified flow formulation. SD3 outperforms state-of-the-art text-to-image generation systems.',
      created_at: '2024-01-11T16:00:00Z',
      fetched_at: '2024-01-11T18:00:00Z',
    },
  },
];

// å°çº¢ä¹¦é£æ ¼æ¨¡æ¿
const xhsTemplates = {
  titles: [
    'ğŸ”¥ éœ‡æƒŠï¼{topic}ç«Ÿç„¶èƒ½åšåˆ°è¿™æ ·ï¼',
    'ğŸ’¥ è°·æ­Œ/Metaæœ€æ–°ç ”ç©¶ï¼Œ{topic}è¦å˜å¤©äº†ï¼',
    'âœ¨ å§å¦¹ä»¬ï¼è¿™ä¸ª{topic}å¤ªç»äº†ï¼',
    'ğŸ“¢ é‡ç£…ï¼{topic}é¢†åŸŸçš„é‡Œç¨‹ç¢‘ä¹‹ä½œï¼',
    'ğŸš€ AIåœˆéƒ½åœ¨ç–¯ä¼ çš„{topic}ï¼Œç»ˆäºææ‡‚äº†ï¼',
  ],
  intros: [
    'å§å¦¹ä»¬ï¼ä»Šå¤©å¿…é¡»ç»™ä½ ä»¬åˆ†äº«ä¸€ä¸ªè¶…çº§é‡ç£…çš„æ¶ˆæ¯ ğŸ’¥',
    'å®¶äººä»¬ï¼è¿™ç¯‡è®ºæ–‡çœŸçš„å¤ªé¡¶äº†ï¼Œä¸çœ‹åæ‚”ç³»åˆ— ğŸ”¥',
    'AI åœˆæœ€è¿‘éƒ½åœ¨è®¨è®ºè¿™ä¸ªï¼Œæˆ‘æ¥ç»™å¤§å®¶ç¿»è¯‘ç¿»è¯‘ ğŸ‘€',
    'ç ä½ï¼è¿™å¯èƒ½æ˜¯ä»Šå¹´æœ€é‡è¦çš„ AI ç ”ç©¶ä¹‹ä¸€ ğŸ“š',
    'ç»ˆäºç­‰åˆ°äº†ï¼è¿™ç¯‡è®ºæ–‡æˆ‘è¿½äº†å¥½ä¹… âœ¨',
  ],
  endings: [
    'ğŸ“ é‡ç‚¹æ¥äº†',
    'ğŸ’¡ åˆ’é‡ç‚¹',
    'â­ æ ¸å¿ƒè¦ç‚¹',
    'ğŸ¯ ä¸€å¥è¯æ€»ç»“',
    'ğŸ’ª å®ç”¨å»ºè®®',
  ],
  tags: [
    'AIè®ºæ–‡', 'äººå·¥æ™ºèƒ½', 'æ·±åº¦å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ', 'ChatGPT',
    'å¤§æ¨¡å‹', 'LLM', 'ç§‘æŠ€å‰æ²¿', 'AIå¹²è´§', 'æŠ€æœ¯åˆ†äº«',
  ],
};

// ç”Ÿæˆæ¨¡æ‹Ÿçš„å°çº¢ä¹¦å†…å®¹
export function generateMockXHSContent(paper: RawPaper): XHSContent {
  const topic = paper.paper_title?.split(':')[0] || 'AI';
  
  // éšæœºé€‰æ‹©æ¨¡æ¿
  const titleTemplate = xhsTemplates.titles[Math.floor(Math.random() * xhsTemplates.titles.length)];
  const intro = xhsTemplates.intros[Math.floor(Math.random() * xhsTemplates.intros.length)];
  const ending = xhsTemplates.endings[Math.floor(Math.random() * xhsTemplates.endings.length)];
  
  const title = titleTemplate.replace('{topic}', topic);
  
  const content = `${intro}

ä»Šå¤©è¦èŠçš„æ˜¯ã€Œ${paper.paper_title}ã€è¿™ç¯‡è®ºæ–‡ ğŸ“„

ç®€å•æ¥è¯´ï¼Œè¿™ç¯‡è®ºæ–‡åšäº†ä»€ä¹ˆå‘¢ï¼Ÿ

${paper.paper_abstract?.slice(0, 200)}...

${ending}ï¼š
âœ… æŠ€æœ¯åˆ›æ–°ç‚¹ï¼šæå‡ºäº†å…¨æ–°çš„æ¶æ„/æ–¹æ³•
âœ… æ•ˆæœï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°SOTA
âœ… å½±å“ï¼šå¯èƒ½æ”¹å˜æ•´ä¸ªè¡Œä¸šçš„æ–¹å‘

è¿™ç¯‡è®ºæ–‡çš„ç‚¹èµæ•°å·²ç»è¶…è¿‡ ${paper.like_count} äº†ï¼Œè¯´æ˜ç¡®å®å¾ˆæœ‰ä»·å€¼ ğŸ”¥

æƒ³æ·±å…¥äº†è§£çš„å®å­å¯ä»¥å»çœ‹åŸæ–‡å“¦~
è®ºæ–‡é“¾æ¥æˆ‘æ”¾è¯„è®ºåŒºå•¦ ğŸ‘‡

#AIè®ºæ–‡ #äººå·¥æ™ºèƒ½ #æ·±åº¦å­¦ä¹  #ç§‘æŠ€å‰æ²¿ #å¹²è´§åˆ†äº«`;

  // éšæœºé€‰æ‹© 3-5 ä¸ªæ ‡ç­¾
  const shuffledTags = [...xhsTemplates.tags].sort(() => Math.random() - 0.5);
  const selectedTags = shuffledTags.slice(0, 3 + Math.floor(Math.random() * 3));

  return {
    id: `xhs_${paper.id}`,
    paper_id: paper.id,
    title,
    content,
    tags: selectedTags,
    cover_text: [topic, 'æ·±åº¦è§£è¯»', 'å»ºè®®æ”¶è—', 'ğŸ“š'],
    emoji_list: ['ğŸ”¥', 'ğŸ’¥', 'âœ¨', 'ğŸ“š', 'ğŸ‘€', 'ğŸ’¡', 'âœ…'],
    created_at: new Date().toISOString(),
    updated_at: new Date().toISOString(),
  };
}
